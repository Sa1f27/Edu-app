<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech-to-Speech Conversation Test</title>
    <style>
        body {
            margin: 0;
            padding: 20px;
            font-family: Arial, sans-serif;
            background-color: #f5f5f5;
            line-height: 1.6;
        }
        
        .container {
            max-width: 600px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            padding: 30px;
        }
        
        h1 {
            text-align: center;
            color: #333;
            margin-bottom: 30px;
        }
        
        .status-box {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
        }
        
        .status-title {
            font-weight: bold;
            color: #495057;
            margin-bottom: 10px;
        }
        
        .status-message {
            font-size: 16px;
            color: #6c757d;
        }
        
        button {
            display: block;
            margin: 20px auto;
            padding: 15px 30px;
            font-size: 16px;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
        }
        
        .start-btn {
            background-color: #007bff;
            color: white;
        }
        
        .start-btn:hover {
            background-color: #0056b3;
        }
        
        .restart-btn {
            background-color: #28a745;
            color: white;
        }
        
        .restart-btn:hover {
            background-color: #1e7e34;
        }
        
        button:disabled {
            background-color: #6c757d;
            cursor: not-allowed;
        }
        
        .recording-indicator {
            display: none;
            align-items: center;
            justify-content: center;
            gap: 10px;
            margin: 20px 0;
            color: #dc3545;
            font-weight: bold;
        }
        
        .recording-dot {
            width: 12px;
            height: 12px;
            background-color: #dc3545;
            border-radius: 50%;
            animation: pulse 1s infinite;
        }
        
        .preparation-indicator {
            display: none;
            align-items: center;
            justify-content: center;
            gap: 15px;
            margin: 20px 0;
            color: #ffc107;
            font-weight: bold;
            font-size: 18px;
        }
        
        .countdown-circle {
            width: 50px;
            height: 50px;
            border: 4px solid #ffc107;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 20px;
            font-weight: bold;
            animation: countdown-pulse 1s infinite;
            background: white;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.3; }
        }
        
        @keyframes countdown-pulse {
            0%, 100% { 
                transform: scale(1);
                border-color: #ffc107;
                background: white;
            }
            50% { 
                transform: scale(1.1);
                border-color: #ff6b35;
                background: #fff8e1;
            }
        }
        
        .test-id {
            margin-top: 20px;
            padding: 10px;
            background-color: #e9ecef;
            border-radius: 4px;
            font-size: 12px;
            text-align: center;
            color: #6c757d;
        }
        
        .log {
            background: #212529;
            color: #00ff00;
            padding: 15px;
            border-radius: 4px;
            font-family: monospace;
            font-size: 12px;
            height: 200px;
            overflow-y: auto;
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸŽ¤ Speech Conversation Test</h1>
        
        <div class="status-box">
            <div class="status-title" id="statusTitle">READY</div>
            <div class="status-message" id="statusMessage">Click Start to begin automatic conversation</div>
        </div>

        <div class="recording-indicator" id="recordingIndicator">
            <div class="recording-dot"></div>
            <span>Recording... Speak now!</span>
        </div>

        <div class="preparation-indicator" id="preparationIndicator">
            <div class="countdown-circle" id="countdownCircle">5</div>
            <span>Get ready to answer... Recording starts in <span id="countdownText">5</span> seconds</span>
        </div>

        <div id="summaryResults" class="status-box" style="display: none; text-align: left;">
            <div class="status-title">Test Evaluation</div>
            <pre id="summaryText" style="white-space: pre-wrap; word-wrap: break-word;"></pre>
            <a id="pdfLink" href="#" target="_blank" style="display: none;">Download Full Report (PDF)</a>
        </div>

        <button id="controlBtn" class="start-btn" onclick="startConversation()">
            ðŸŽ¤ Start Conversation
        </button>

        <div id="testIdDisplay" class="test-id" style="display: none;"></div>
        
        <div class="log" id="logOutput"></div>
    </div>

    <script>
        // Global variables
        let conversationState = 'idle'; // idle, starting, listening, processing, speaking, ended, preparing
        let testId = null;
        let isRecording = false;
        let isPreparingToRecord = false;
        let preparationCountdown = 0;
        
        // Audio recording variables
        let mediaRecorder = null;
        let audioChunks = [];
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let silenceStart = null;
        let hasSpoken = false;
        
        // Timer references
        let preparationTimer = null;
        
        // Configuration
        const API_BASE_URL = window.location.origin + '/daily_standup';
        const SILENCE_THRESHOLD = 0.01;
        const SILENCE_DURATION = 2000; // 2 seconds
        const MAX_RECORDING_DURATION = 15000; // 15 seconds
        const PREPARATION_TIME = 1000; // 5 seconds preparation before recording
        
        // DOM elements
        const statusTitle = document.getElementById('statusTitle');
        const statusMessage = document.getElementById('statusMessage');
        const recordingIndicator = document.getElementById('recordingIndicator');
        const preparationIndicator = document.getElementById('preparationIndicator');
        const countdownCircle = document.getElementById('countdownCircle');
        const countdownText = document.getElementById('countdownText');
        const controlBtn = document.getElementById('controlBtn');
        const testIdDisplay = document.getElementById('testIdDisplay');
        const logOutput = document.getElementById('logOutput');
        const summaryResults = document.getElementById('summaryResults');
        const summaryText = document.getElementById('summaryText');
        const pdfLink = document.getElementById('pdfLink');

        // Utility functions
        function log(message, type = 'info') {
            const timestamp = new Date().toLocaleTimeString();
            const color = type === 'error' ? '#ff6b6b' : type === 'success' ? '#00d2d3' : '#00ff00';
            logOutput.innerHTML += `<div style="color: ${color}">[${timestamp}] ${message}</div>`;
            logOutput.scrollTop = logOutput.scrollHeight;
            console.log(`[${timestamp}] ${message}`);
        }

        function updateStatus(title, message) {
            statusTitle.textContent = title;
            statusMessage.textContent = message;
            log(`Status: ${title} - ${message}`);
        }

        function updateConversationState(newState) {
            conversationState = newState;
            log(`State changed to: ${newState}`);
        }

        // Audio utility functions
        function calculateRMS(buffer) {
            let sum = 0;
            for (let i = 0; i < buffer.length; i++) {
                sum += buffer[i] * buffer[i];
            }
            return Math.sqrt(sum / buffer.length);
        }

        function detectSilence() {
            if (!isRecording || !analyser) return;

            const dataArray = new Float32Array(analyser.frequencyBinCount);
            analyser.getFloatTimeDomainData(dataArray);
            const rms = calculateRMS(dataArray);
            const isSilent = rms < SILENCE_THRESHOLD;
            const now = Date.now();

            if (isSilent) {
                if (silenceStart === null) {
                    silenceStart = now;
                } else if (hasSpoken && (now - silenceStart) > SILENCE_DURATION) {
                    log('Silence detected for 2 seconds, stopping recording', 'success');
                    stopRecording();
                    return;
                }
            } else {
                silenceStart = null;
                if (!hasSpoken && rms > SILENCE_THRESHOLD * 3) {
                    hasSpoken = true;
                    log('User started speaking', 'success');
                }
            }

            if (isRecording) {
                requestAnimationFrame(detectSilence);
            }
        }

        function playAudio(audioPath) {
            return new Promise((resolve) => {
                if (!audioPath) {
                    resolve();
                    return;
                }
                
                const audio = new Audio(`${API_BASE_URL}${audioPath}`);
                audio.onended = resolve;
                audio.onerror = (e) => {
                    log(`Audio playback error: ${e}`, 'error');
                    resolve();
                };
                
                audio.play().catch((e) => {
                    log(`Audio play failed: ${e}`, 'error');
                    resolve();
                });
            });
        }

        // Cleanup function
        function cleanup() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
            if (audioContext) {
                audioContext.close();
            }
            if (microphone) {
                microphone.disconnect();
            }
            if (preparationTimer) {
                clearInterval(preparationTimer);
                preparationTimer = null;
            }
            isRecording = false;
            isPreparingToRecord = false;
            preparationCountdown = 0;
            recordingIndicator.style.display = 'none';
            preparationIndicator.style.display = 'none';
        }

        // Start preparation phase before recording
        function startPreparationPhase() {
            if (isPreparingToRecord || isRecording || conversationState === 'ended') return;

            try {
                log('Starting 5-second preparation phase');
                isPreparingToRecord = true;
                preparationCountdown = 5;
                
                updateConversationState('preparing');
                updateStatus('PREPARING', `Get ready to answer... Recording starts in ${preparationCountdown} seconds`);
                
                // Show preparation indicator
                preparationIndicator.style.display = 'flex';
                countdownCircle.textContent = preparationCountdown;
                countdownText.textContent = preparationCountdown;
                
                preparationTimer = setInterval(() => {
                    preparationCountdown--;
                    countdownCircle.textContent = preparationCountdown;
                    countdownText.textContent = preparationCountdown;
                    updateStatus('PREPARING', `Get ready to answer... Recording starts in ${preparationCountdown} seconds`);
                    
                    if (preparationCountdown <= 0) {
                        clearInterval(preparationTimer);
                        preparationTimer = null;
                        isPreparingToRecord = false;
                        preparationIndicator.style.display = 'none';
                        
                        if (conversationState !== 'ended') {
                            log('Preparation phase complete, starting recording');
                            startRecording();
                        }
                    }
                }, 1000);
                
            } catch (error) {
                log(`Preparation phase error: ${error.message}`, 'error');
                isPreparingToRecord = false;
                preparationIndicator.style.display = 'none';
                startRecording(); // Fallback to immediate recording
            }
        }

        // Fetch and display summary
        async function fetchAndDisplaySummary() {
            if (!testId) {
                log('No test ID available to fetch summary.', 'error');
                return;
            }

            try {
                log(`Fetching summary for test ID: ${testId}`);
                updateStatus('EVALUATING', 'Please wait while we generate your evaluation...');

                const response = await fetch(`${API_BASE_URL}/summary?test_id=${testId}`);
                const data = await response.json();

                if (!response.ok) {
                    throw new Error(data.error || 'Failed to fetch summary');
                }

                log('Summary received successfully', 'success');
                
                // Display summary
                summaryText.textContent = data.summary;
                if (data.pdf_url) {
                    pdfLink.href = `${API_BASE_URL}${data.pdf_url}`;
                    pdfLink.style.display = 'block';
                }
                summaryResults.style.display = 'block';

            } catch (error) {
                log(`Error fetching summary: ${error.message}`, 'error');
                summaryText.textContent = `Could not retrieve evaluation: ${error.message}`;
                summaryResults.style.display = 'block';
            }
        }

        // Start recording with silence detection
        async function startRecording() {
            if (isRecording || isPreparingToRecord || conversationState === 'ended') return;

            try {
                log('Starting recording with silence detection');
                isRecording = true;
                hasSpoken = false;
                silenceStart = null;
                audioChunks = [];
                
                updateConversationState('listening');
                updateStatus('LISTENING', 'Please speak your response...');
                recordingIndicator.style.display = 'flex';

                // Get user media
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 16000
                    }
                });

                // Set up audio analysis for silence detection
                audioContext = new AudioContext({ sampleRate: 16000 });
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                
                analyser.fftSize = 256;
                analyser.smoothingTimeConstant = 0.8;
                
                microphone.connect(analyser);

                // Set up MediaRecorder
                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'audio/webm;codecs=opus'
                });

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    log('MediaRecorder stopped, processing audio');
                    
                    // Clean up audio resources
                    if (microphone) {
                        microphone.disconnect();
                    }
                    if (audioContext) {
                        await audioContext.close();
                    }
                    stream.getTracks().forEach(track => track.stop());

                    recordingIndicator.style.display = 'none';

                    // Process the recorded audio
                    if (audioChunks.length > 0) {
                        await processRecordedAudio();
                    } else {
                        log('No audio data recorded', 'error');
                        if (conversationState !== 'ended') {
                            updateStatus('ERROR', 'No audio detected. Retrying...');
                            setTimeout(() => startPreparationPhase(), 1000);
                        }
                    }
                };

                // Start recording
                mediaRecorder.start(100);

                // Start silence detection
                detectSilence();

                // Failsafe: stop recording after max duration
                setTimeout(() => {
                    if (isRecording) {
                        log('Max recording duration reached, stopping', 'error');
                        stopRecording();
                    }
                }, MAX_RECORDING_DURATION);

            } catch (error) {
                log(`Recording error: ${error.message}`, 'error');
                updateStatus('ERROR', `Recording failed: ${error.message}`);
                endConversation('Recording error');
            }
        }

        // Stop recording
        function stopRecording() {
            if (!isRecording) return;
            
            log('Stopping recording');
            isRecording = false;
            
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
        }

        // Process recorded audio
        async function processRecordedAudio() {
            try {
                updateConversationState('processing');
                updateStatus('PROCESSING', 'Processing your response...');
                
                // Create blob from recorded chunks
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                
                // Check if we have sufficient audio data
                if (audioBlob.size < 1000) {
                    log('Recording too short, requesting user to speak again');
                    updateStatus('LISTENING', 'Recording was too short. Please speak again...');
                    setTimeout(() => startPreparationPhase(), 1000);
                    return;
                }

                // Send to backend for processing
                const formData = new FormData();
                formData.append('audio', audioBlob, 'response.webm');
                formData.append('test_id', testId);

                log('Sending audio to backend...');
                const response = await fetch(`${API_BASE_URL}/record_and_respond`, {
                    method: 'POST',
                    body: formData,
                });

                const data = await response.json();
                log('Backend response received');

                if (!response.ok) {
                    throw new Error(data.error || 'Recording processing failed');
                }

                const responseText = data.response || 'AI response not available';
                log(`AI Response: ${responseText}`);
                
                updateConversationState('speaking');
                updateStatus('AI SPEAKING', responseText);
                
                // Play AI response audio
                if (data.audio_path) {
                    log(`Playing AI audio: ${data.audio_path}`);
                    await playAudio(data.audio_path);
                }

                if (data.ended) {
                    await fetchAndDisplaySummary(); // Fetch summary first
                    endConversation('Test completed successfully'); // Then end the conversation flow
                } else if (conversationState !== 'ended') {
                    // Continue with next question cycle
                    updateStatus('WAITING', 'Please respond...');
                    setTimeout(() => startPreparationPhase(), 1000);
                }

            } catch (error) {
                log(`Error processing audio: ${error.message}`, 'error');
                updateStatus('ERROR', `Processing error: ${error.message}`);
                endConversation('Processing error');
            }
        }

        // Start conversation
        async function startConversation() {
            try {
                updateConversationState('starting');
                updateStatus('STARTING', 'Initializing conversation...');
                controlBtn.disabled = true;
                
                log('Starting new conversation...');
                const response = await fetch(`${API_BASE_URL}/start_test`);
                
                if (!response.ok) {
                    throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                }
                
                const data = await response.json();
                log('Test started successfully');
                
                if (!data.test_id) {
                    throw new Error('No test ID received from server');
                }
                
                testId = data.test_id;
                testIdDisplay.textContent = `Test ID: ${testId}`;
                testIdDisplay.style.display = 'block';
                
                const questionText = data.question || 'Please tell me about today\'s topic.';
                
                updateConversationState('speaking');
                updateStatus('AI SPEAKING', questionText);
                log(`First question: ${questionText}`);
                
                // Play initial question audio
                if (data.audio_path) {
                    await playAudio(data.audio_path);
                }
                
                // Start listening for user response with preparation phase
                if (conversationState !== 'ended') {
                    updateStatus('WAITING', 'Please respond...');
                    setTimeout(() => startPreparationPhase(), 500);
                }
                
            } catch (error) {
                log(`Start conversation error: ${error.message}`, 'error');
                updateStatus('ERROR', `Failed to start: ${error.message}`);
                endConversation('Startup error');
            }
        }

        // End conversation
        function endConversation(reason) {
            updateConversationState('ended');
            updateStatus('ENDED', `Conversation ended: ${reason}`);
            
            cleanup();
            
            controlBtn.disabled = false;
            controlBtn.textContent = 'ðŸ”„ Start New Conversation';
            controlBtn.className = 'restart-btn';
            controlBtn.onclick = restartConversation;
            
            log(`Conversation ended: ${reason}`, 'success');
        }

        // Restart conversation
        function restartConversation() {
            updateConversationState('idle');
            updateStatus('READY', 'Click Start to begin new conversation');
            
            testId = null;
            testIdDisplay.style.display = 'none';
            summaryResults.style.display = 'none'; // Hide summary box
            preparationIndicator.style.display = 'none'; // Hide preparation indicator
            
            controlBtn.textContent = 'ðŸŽ¤ Start Conversation';
            controlBtn.className = 'start-btn';
            controlBtn.onclick = startConversation;
            
            log('Ready for new conversation');
        }

        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            cleanup();
        });

        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            log('Speech Conversation Test loaded', 'success');
            log('Ready to start conversation flow test');
        });
    </script>
</body>
</html>